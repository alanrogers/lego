\mainpage

# Introduction

Legofit is a computer package that uses counts of nucleotide site
patterns to estimate the history of population size, subdivision, and
gene flow. The package consists of the following programs

* @ref daf "daf", which writes genetic data into the ".daf" format
  that is used by @ref tabpat "tabpat".
* @ref tabpat "tabpat", which reads ".daf" files for several
  populations, tabulates "nucleotide site patterns" (explained below),
  and generates moving-blocks bootstrap replicates.
* @ref legosim "legosim", which predicts site pattern counts from
  assumptions about population history.
* @ref legofit "legofit", which estimates parameters from site pattern
  counts.
* @ref bootci "bootci.py", which uses multiple legofit output files
  (one for the real data and one for each bootstrap replicate) to
  generate bootstrap confidence intervals for estimated parameters.
* @ref flatfile "flatfile.py", which reads a list of legofit output
  files and writes a flat file with a row for each legofit file and a
  column for each parameter.
* @ref diverg "diverg.py", which compares two distributions of site
  pattern frequencies, using the Kullback-Leibler (KL) divergence.

# Nucleotide site patterns {#sitepat}

These programs all operate on "nucleotide site patterns", which are
summary statistics not influenced by recent population size but rich
in information about ancestral populations. This section will define
site patterns and explain how legofit tabulates their counts, and
estimates expectations.

Consider a sample consisting of one haploid genome drawn from each of
3 populations, *X*, *Y*, and *Z*. Suppose that, at a given nucleotide
site, the derived allele is present in the genomes from *X* and *Y*
but not that from *Z*. If so, then this nucleotide position will be
said to exhibit the "*xy* site pattern." We ignore cases in which the
derived allele is present in none of the samples or in all of them.
By default (without the `-1` argument of @ref legosim "legosim" and @ref
legofit "legofit"), we also ignore "singleton site patterns"--those in
which the derived allele appears only once. In other words, we
consider only polymorphic, non-singleton site patterns. For the
special case of the 3-population sample just described, there are only
3 such site patterns: *xy*, *xz*, and *yz*.

In the general case, with samples from *K* populations, the number of
site patterns is \f$2^K - K - 2\f$. For example, there are 10 site
patterns in a sample involving \f$K=4\f$ populations. The table below
shows data from a sample involving 4 populations, *X*, *Y*, *N*, and
*D*.

    SitePat          E[count]
         xy    340952.4592501
         xn     46874.1307236
         xd     46034.4670204
         yn     55137.4236715
         yd     43535.5248078
         nd    231953.3372578
        xyn     91646.1277991
        xyd     88476.9619569
        xnd     96676.3877423
        ynd    100311.4411513

The `E[count]` column shows numbers that can be thought of loosely as
counts of site patterns in a genome-wide sample. The last line tells
us that the *ynd* site pattern occurs at over 100,000 nucleotide
sites.

These number cannot really be counts, because they aren't
integers. This reflects the fact that our sample includes more than
one haploid genome from each population, and a given SNP may
contribute to several site patterns. The contribution to a given site
pattern is the probability that a sub-sample, consisting of one
haploid genome drawn at random from the larger sample of each
population, would exhibit this site pattern. For example, suppose we
have samples from three populations, *X*, *Y*, and *N*, and let
\f$p_{iX}\f$, \f$p_{iY}\f$, and \f$p_{iN}\f$ represent the frequencies
of the derived allele at the *i*th SNP in these three
samples. Then site pattern *xy* occurs at SNP *i* with
probability \f$z_i = p_{iX}p_{iY}(1-p_{iN})\f$ (Patterson et al 2010,
Science, 328(5979):S129).  Aggregating over SNPs, \f$I_{xy} = \sum_i
z_i\f$ summarizes the information in the data about this site
pattern. These are the numbers that appear in the 2nd column of the
table above.

The table above was generated by the program @ref tabpat "tabpat".

# Installation and testing

The package is available at
[github](https://github.com/alanrogers/legofit). Before compiling, you must
install two libraries: `pthreads` and
[`gsl`](http://www.gnu.org/software/gsl). You will need not only the
libraries themselves but also several header files, such as
`pthread.h`. I didn't need to install `pthreads`, because it came
bundled with the Gnu C compiler. But the gsl was an extra. Under
ubuntu Linux, you can install it like this:

    sudo apt-get install libgsl0-dev

On the mac, using homebrew, the command is

    brew install gsl

By default, the executable files will be copied into a directory named
`bin` in your home directory. If you want them to go somewhere else,
edit the first non-comment line of src/Makefile.

Then

1. Cd into the src directory.
2. Type "make".
3. Type "make install".

This will try to place the executables into directory "bin" in the
user's home directory. Make sure this directory appears in your
PATH, so that the shell can find it.

There is also a facility for installing a "global" version of the code
for other users. The first step (under unix-like operating systems) is
to set up a "soft link" to the directory just above the target bin
directory. Then type "sudo make ginstall". You'll need administrative
privileges. For example, to install the software in `/usr/local/bin`:

    cd                     # move into home directory
	ln -s /usr/local group # now "group" points to `/usr/local`

Then `cd` back into the `legofit` directory and type

    sudo make ginstall     # install into /usr/local/bin

This installation will work under unix-like operating systems, such as
linux and Apple's osx. I haven't tried to port this software to
Windows.

The directory `test` contains a unit test for many of the .c files in
directory `src`. Within this directory, type

1. make xboot
2. ./xboot

to test the source file `boot.c`.  To run all unit tests, type
"make". This will take awhile, as some of the unit tests are slow.

# Genetic input data

Before doing data analysis with `legofit`, you must generate data files
in "daf" format. Such files end with ".daf", which stands for "derived
allele frequency. See the @ref daf "daf" command for instructions on
translating from "vcf" or "bcf" format into "daf".

The "daf" file is very simple and looks like this:

    #chr        pos aa da                  daf
       1     752566  g  a 0.835294117647058854
       1     754192  a  g 0.858823529411764652
       1     755225  t  g 0.000000000000000000
       1     755228  t  g 0.000000000000000000
       1     765437  g  a 0.000000000000000000

The first line (beginning with "#") is an optional comment, which is
used here to label the columns. The columns are as follows:

1. Character strings that label chromosomes or scaffolds.
2. Position of the SNP on the chromosome or scaffold, measured in base
   pairs. Daf format doesn't care whether nucleotide positions are
   numbered beginning with 0 or with 1, provided that they are consistent
   across files in a given analysis.
4. Ancestral allele, a single letter.
5. Derived allele, also a single letter. Loci with 3 or more alleles
   should be excluded.
6. Frequency of the derived allele within the sample.

The lines should be sorted lexically by chromosome. Within
chromosomes, they should be sorted in ascending numerical order of
column 2.

# Describing population history in an lgo file {#lgo}

The ".lgo" format describes the history of population size,
subdivision, and gene flow. It also identifies the position within the
population network of each genetic sample. It is a plain-text file,
which should be constructed with a text editor.

In the paragraphs below, I will introduce a small .lgo file a few
lines at a time. The first line in my file is

    # Example .lgo file

This is a comment. Comments extend from the sharp character to the end
of the line. The next two lines define two convenience variables,
whose names are "zero" and "one"

    time fixed    zero=0
    twoN fixed     one=1

The first is a "time" variable, which I will use for the tips of
branches, where time equals 0. I declare it "fixed", which means that
it will not change. The second is a "twoN" variable, which represents
twice the size of a population. When there is only one sample per
population, the sizes of tip populations do not matter, so I set
them all equal to "one". Next, three more time variables named "Txyn",
"Tn", and "Txy".

    time fixed    Tn=1897          # time of Neanderthal admixture
    time free     Txyn=25920       # archaic-modern separation time
    time free     Txy=3788         # Africa-Eurasia separation time

The comments hint at the meanings of these variables; the values are
in generations. The first of these variables is "fixed" (see
above). The other two are "free", which means that `legofit` will
estimate their values.

It is also possible to define variables as "gaussian". For example,

    time gaussian x=123 sd=10

Gaussian variables are used to represent exogeneous parameters whose
values are known only approximately. They are modeled as Gaussian
random variables. In this case, the mean is 123 and the standard
deviation is 10. Programs `legosim` and `legofit` use Monte-Carlo
integration to integrate across the uncertainty in Gaussian
parameters. Although the package still supports Gaussian parameters, I
don't recommend them, for reasons discussed below. The Gaussian
variable "x" defined will be ignored in what follows.

Our measure of population size is twice the effective size of the
population, and we define two such variables:

    twoN free     2Nn=1000          # archaic population size
    twoN constrained 2Nxy=10000 + -1.2*Txy # early modern population size

The first of these is a free parameter, but the second is a new
category: "constrained". It defines "2Nxy" as a function of "Txy".
Constraints are useful when analysis of bootstrap samples indicates a
tight relationship between two or more free parameters. Constraints
reduce the number of free parameters and allow more accurate
estimates. In the constraint above, there are only two terms and one
independent variable--"Txy". It is legal, however, to use any number of
terms and independent variables. For example, we could have written

    twoN constrained 2Nxy=10000 + -1.2*Txy + 0.01*Txy*Txyn # OK

All independent variables must be free parameters, and all must be
defined before the constraint. The terms in the constraint must be
separated by "+". It would *not* be legal to rewrite the constraint
as

    twoN constrained 2Nxy=10000 - 1.2*Txy # ERROR

We have one more variable to declare:

    mixFrac free  mN=0.02           # Neanderthal admixture into y

The "mixFrac" command declares a "mixture fraction"--the fraction of a
some population that derives from introgression. As above, it could
have been fixed, gaussian, or constrained.

The next few lines of the input file declare the segments of the
population network. The first of these is

    segment x     t=zero   twoN=one    samples=1  # Africa

Here, "x" is the name of the segment, zero is the time at which it
ends, and one is the population size. Note that "zero" and "one" are
variables that we declared above. The "samples=1" phrase says that
there is a genetic sample from the end of this segment. In other
words, the date of the sample is "t=zero". If the segment has no
samples, you can omit "samples=0". It is also possible to specify more
than one sample, as in "samples=2". If you do this, the program will
generate more complex site patterns, whose frequencies will depend on
recent population size. In this situation, you would not want to set
"twoN=one".

The next two lines are similar, and define two other terminal
populations:

    segment y     t=zero   twoN=one    samples=1  # Eurasia
    segment n     t=Tn     twoN=2Nn    samples=1  # Neanderthal

Segment "n" does not end at time zero, but rather at the time, Tn, of
Neanderthal admixture. It has one sample, whose date is also Tn. This
is a bit of a stretch, because it assumes that the Neanderthal genome
lived at the same time as the episode of admixture. I make this
assumption for simplicity--this is only an example. There are 3 more
segments to declare:

    segment y2    t=Tn     twoN=one               # pre-mig eurasia
    segment xy    t=Txy    twoN=2Nxy              # early modern
    segment xyn   t=Txyn   twoN=2Nn               # ancestral

These segments don't have a "samples" component, because none of them
have genetic samples.  Segment y2 represents the Eurasian population
before the episode of admixture. Note that it ends at the same time as
segment n. This is necessary, because we will want to mix y2 and n
below to model gene flow. Also note that the size of xyn equals
2Nn--the same variable we used in setting the size of segment n. This
establishes a constraint: the sizes of XYN and N will always be equal,
no matter how the optimizer adjusts the value of 2Nn.

The rest of the .lgo file defines relationships between segments. This
involves two statements: "mix" and "derive". Consider the mix
statement first:

    mix    y  from y2 + mN * n      # y is a mixture of y2 and n

This says that y is a mixture of y2 and n, which must end at the same
date. Specifically, a fraction mN of y comes from n and the remaining
fraction comes from y2. Finally, we have 4 examples of the "derive"
statement:

    derive x  from xy               # x is child of xy
    derive y2 from xy               # y2 is child of xy
    derive xy from xyn              # xy is child of xyn
    derive n  from xyn              # n is child of xyn

These statements establish ancestor-descendant relationships between
segments. Note that x and y2 both derive from xy, so xy has two
"children", but x and y2 each have only one parent. On the other hand,
segment y has two parents, as defined in the "mix" statement above.

Segments cannot have more than two parents or more than two
children. All segments should descend, eventually, from a single
root.

Using this .lgo file as input, `legosim -i 10000` produces

    ############################################################
    # legosim: generate site patterns by coalescent simulation #
    ############################################################
    
    # Program was compiled: Jun  8 2017 12:41:14
    # Program was run: Mon Jul 10 14:06:22 2017
    
    # cmd: legosim -i 1000 input.lgo
    # nreps                       : 1000
    # input file                  : input.lgo
    # not simulating mutations
    # excluding singleton site patterns.
    #       SitePat E[BranchLength]
                x:y   17466.0903975
                x:n       4.3453445
                y:n     367.5397837

The program reports the mean branch length in generations of three
site patterns. For example, "x:y" refers to the pattern in which the
derived allele is present in the samples x and y but not in n. The 2nd
column gives the expected length in generations of the branch on which
mutations would generate the corresponding site pattern.

# Using the package

## Predicting site pattern counts from assumptions about history

For this purpose, you want to use the program @ref legosim "legosim". The
first step is create a file in @ref lgo ".lgo" format, which describes
the history of population size, subdivision, and gene flow. This
format is described above. Then, you can execute `legosim` by typing:

    legosim -i 10000 my_input_file.lgo

See the @ref legosim "legosim" documentation for details.

## Estimating parameters from genetic data

This involves several programs. The first step is to generate input
files in ".daf" format, as described above. You will need one .daf
file for each population. See the @ref daf "daf" documentation for
details.

The next task is to tabulate site pattern counts from the various .daf
files. See the @ref tabpat "tabpat" documentation for details. Tabpat
will generate a small text file, with one row for each site
pattern. If there are 4 populations in the analysis, there will be 10
site patterns.

You will also need a .lgo file, which describes the model of history
you wish to explore. It also specifies which parameters will be
estimated. Details are above.

Finally, @ref legofit "legofit" will estimate parameters. This program
may take several hours to run, depending on the size of the analysis
and the number of simulation replicates used to approximate the
objective function.

To generate a bootstrap confidence interval, use the `--bootreps`
option of @ref tabpat "tabpat". This will generate not only the
primary output (written to standard output), but also an additional
output file for each bootstrap replicate. For example, `--bootreps 50`
would generate 51 output files: 1 for the normal output, and 1 for
each bootstrap replicate.

These files should each be analyzed with a separate run of
`legofit`. If you have access to a compute cluster, these jobs can be
run in parallel.

You might be tempted to parallelize these legofit jobs on a single
computer, using multiple threads of execution. Don't do it. A single
legofit job makes use of all available threads, so nothing is gained
by launching simultaneous legofit jobs on a single computer. Unless
you have access to a compute cluster, legofit jobs should be run
sequentially. As each job can take several hours, a full bootstrap may
take several days.

Having run `legofit` on the real data and all bootstrap replicates,
you can use @ref bootci "bootci.py" and @ref flatfile "flatfile.py" to
summarize the information in the resulting output files.

Finally, @ref diverg "diverg.py" can be used for comparing two sets of
site-pattern counts or frequencies. It uses the Kullback-Leibler (KL)
divergence to measure the discrepancy between the two
distributions. It provides not only the overall KL divergence but also
the contribution of each site pattern. Thus, it will tell you which
site patterns are responsible for a poor fit between observed and
expected site pattern frequencies.

# Bias in `legofit`

Biases arise in `legofit` because of the constraint that a child
population cannot be older than its parent. If the parent's age is
fixed, then it represents an inequality constraint on the age of the
child. If the child is only slightly younger than the parent, the
optimizer has problems. It will propose new values for the child's
age. Those that increase the child's age will be rejected if they
violate the inequality constraint. Those that decrease the child's age
do not encounter this constraint. Consequently, the optimizer is more
likely to move away from the constraint rather than toward it. This
biases estimates away from the constraint, whenever the true value is
close to the constraint.

The magnitude of this bias depends on the spread of the sampling
distribution of the child's age. If that sampling distribution is
wide, then the DE algorithm will tend to take large steps and will
encounter the boundary constraint sooner. This generates a large
bias. If the sampling distribution is narrow, the bias is small.

This bias is exacerbated if the parent's age is modeled as a Gaussian
variable, because then the constraint is an interval rather than a
point, and the optimizer encounters it sooner. For this reason, I have
not used Gaussian variables in recent work.

